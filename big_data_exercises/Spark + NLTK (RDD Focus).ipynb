{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark + NLTK Exercise\n",
    "Spark exercise leveraging NLTK to improve familiarity with PySpark, work on basics of Spark RDD API, practice application of NLTK. This exercise seeks to show the translation of a Python-based script that leverages part-of-speech tagging on a large dataset and convert it to a pyspark-based approach.\n",
    "\n",
    "`%%time` functions included, but results may vary by operating system configuration.\n",
    "_________________________\n",
    "\n",
    "Part-of-speech tagging is a basic introduction to NLP, and will be performed on some New York Times articles. The original script was written by Luke Petschauer and a forked version is available at https://github.com/umsi-data-science/NP_chunking_with_nltk/blob/master/NP_chunking_with_the_NLTK.ipynb. The complete analysis should take about 10 minutes to run.\n",
    "\n",
    "First, let's load libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !update-java-alternatives -l\n",
    "# !update-java-alternatives -s java-1.8.0-openjdk-amd64 > /dev/null 2> &1\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('book')\n",
    "import re\n",
    "import pprint\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original non-Spark Python script, run on a small snippet of practice text for brevity while working through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stellar pitching', 'afloat', 'first half', 'last season', 'encore', 'pennant-winning season', 'lineup', 'pitching', 'thin', 'pitching', 's game', 'pitching', 'anything', '4-2 loss', 'place', 'spot starter', 'deficit', 'lineup', 'starter', 'room', 'ninth inning', 'last-gasp two-run homer', 'reliever', 'streak', 'team']\n"
     ]
    }
   ],
   "source": [
    "# This is the original (non-Spark) script\n",
    "\n",
    "patterns = \"\"\"\n",
    "    NP: {<JJ>*<NN*>+}\n",
    "    {<JJ>*<NN*><CC>*<NN*>+}\n",
    "    \"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(patterns)\n",
    "\n",
    "def prepare_text(input):\n",
    "    sentences = nltk.sent_tokenize(input)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [NPChunker.parse(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def parsed_text_to_NP(sentences):\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP':\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                nps.append(t)\n",
    "    return nps\n",
    "\n",
    "\n",
    "def sent_parse(input):\n",
    "    sentences = prepare_text(str(input))\n",
    "    nps = parsed_text_to_NP(sentences)\n",
    "    return nps\n",
    "\n",
    "\n",
    "text_to_be_analyzed = \"\"\"WASHINGTON - Stellar pitching kept the Mets afloat in the first half of last season despite their offensive woes. But they cannot produce an encore of their pennant-winning season if their lineup keeps floundering while their pitching is nicked, bruised and stretched thin.\n",
    "\"We were going to ride our pitching,\" Manager Terry Collins said before Wednesday’s game. \"But we're not riding it right now. We've got as many problems with our pitching as we do anything.\"\n",
    "Wednesday's 4-2 loss to the Washington Nationals was cruel for the already-limping Mets. Pitching in Steven Matz's place, the spot starter Logan Verrett allowed two runs over five innings. But even that was too large a deficit for the Mets' lineup to overcome against Max Scherzer, the Nationals' starter.\n",
    "\"We're not even giving ourselves chances,\" Collins said, adding later, \"We just can’t give our pitchers any room to work.\"\n",
    "The Mets did not score until the ninth inning, when a last-gasp two-run homer by James Loney off Nationals reliever Shawn Kelley snapped a streak of 23 scoreless innings for the team.\"\"\"\n",
    "\n",
    "\n",
    "nps = sent_parse(text_to_be_analyzed)\n",
    "print(nps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Conversion\n",
    "Now, let's spin up a Spark session for conversion of this script to a Spark-friendly RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test1') \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession \\\n",
    "#     .builder.master(\"local[*]\") \\\n",
    "#     .appName('test1') \\ \n",
    "#     .config(\"spark.sql.catalogImplementation\",\"in-memory\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['URL: http://www.nytimes.com/2016/06/30/sports/baseball/washington-nationals-max-scherzer-baffles-mets-completing-a-sweep.html',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile('data/nytimes/nytimes_news_articles.txt')\n",
    "# show the first two lines of the file\n",
    "text.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r\"\\b[\\w']+\\b\")\n",
    "def pos_tag_counter(line):\n",
    "    toks = nltk.regexp_tokenize(line, TOKEN_RE)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    return postoks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create RDD Pipeline that does the following:\n",
    "\n",
    "1. filters out blank lines \n",
    "2. filters out lines starting with 'URL'\n",
    "3. creates a single list (using flatMap) that applies the pos_tag_counter function to each line\n",
    "4. maps each resulting line to show the part of speech (which is the second element returned from the pos_tag_counter)\n",
    "5. converts each resulting line to a pairRDD with words as keys and values of 1\n",
    "6. reduces the resulting RDD by key, adding up all the 1s (like the lecture and lab examples)\n",
    "7. sorts the resulting list by the counts, in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 1126515),\n",
       " ('IN', 928916),\n",
       " ('NNP', 853093),\n",
       " ('DT', 761492),\n",
       " ('JJ', 498482),\n",
       " ('NNS', 437116),\n",
       " ('VBD', 379509),\n",
       " ('PRP', 282603),\n",
       " ('RB', 271053),\n",
       " ('CC', 231491),\n",
       " ('VB', 223717),\n",
       " ('CD', 187602),\n",
       " ('TO', 187005),\n",
       " ('VBN', 174980),\n",
       " ('VBZ', 169149),\n",
       " ('VBG', 163653),\n",
       " ('VBP', 143368),\n",
       " ('PRP$', 107984),\n",
       " ('MD', 67185),\n",
       " ('WDT', 44582),\n",
       " ('WP', 42406),\n",
       " ('WRB', 33160),\n",
       " ('RP', 29345),\n",
       " ('JJR', 24746),\n",
       " ('NNPS', 18870),\n",
       " ('JJS', 16425),\n",
       " ('EX', 12397),\n",
       " ('RBR', 12286),\n",
       " ('RBS', 5146),\n",
       " ('PDT', 3784),\n",
       " ('FW', 2793),\n",
       " ('WP$', 2329),\n",
       " ('POS', 493),\n",
       " ('UH', 325),\n",
       " ('$', 219),\n",
       " ('LS', 5),\n",
       " (\"''\", 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos_tag_counts = text.filter(lambda line: len(line) > 0) \\   # Filters out blanks\n",
    "#     .filter(lambda line: re.findall('^(?!URL).*', line)) \\   # Filters out lines starting with 'URL'\n",
    "#     .flatMap(pos_tag_counter) \\                              # Create single list using flatMap applying pos_tag_counter\n",
    "#     .map(lambda word: word[1]) \\                             # maps resulting line to show part of speech\n",
    "#     .map(lambda word: (word, 1)) \\                           # convert each resulting line to a pair RDD by key\n",
    "#     .reduceByKey(lambda x, y: x + y) \\                       # reduce results by key, adding up all 1s\n",
    "#     .sortBy(lambda x: x[1], ascending = False)               # sorts resulting list by counts, descending\n",
    "\n",
    "# pos_tag_counts.collect()\n",
    "\n",
    "pos_tag_counts = text.filter(lambda line: len(line) > 0) \\\n",
    "    .filter(lambda line: re.findall('^(?!URL).*', line)) \\\n",
    "    .flatMap(pos_tag_counter) \\\n",
    "    .map(lambda word: word[1]) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "pos_tag_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way to do it, combining steps 4 & 5\n",
    "\n",
    "# pos_tag_counts = text.filter(lambda line: len(line) > 0) \\\n",
    "#     .filter(lambda line: re.findall('^(?!URL).*', line)) \\\n",
    "#     .flatMap(pos_tag_counter) \\\n",
    "#     .map(lambda word: (word[1], 1)) \\\n",
    "#     .reduceByKey(lambda x, y: x + y) \\\n",
    "#     .sortBy(lambda x: x[1], ascending = False)\n",
    "    \n",
    "# pos_tag_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create RDD pipeline to show distribution of length of noun phrases\n",
    "\n",
    "1. Apply (using flatMap) a ```tokenize_chunk_parse``` function to each line in the ```text``` RDD\n",
    "2. Use map to emit the length of each noun phrase\n",
    "3. Use map to convert each resulting line to a pairRDD with words as keys and values of 1\n",
    "4. Reduce the resulting RDD by key, adding up all the 1s (like the lecture and lab examples)\n",
    "5. Sort the resulting list by the counts, in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJS>*<NN.*>}\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}\n",
    "\"\"\"\n",
    "\n",
    "  \n",
    "def tokenize_chunk_parse(line):\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "  \n",
    "    toks = nltk.regexp_tokenize(line, TOKEN_RE)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    tree = chunker.parse(postoks)\n",
    "\n",
    "    return [term for term in leaves(tree)] \n",
    "  \n",
    "def leaves(tree):\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        yield subtree.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1194014),\n",
       " (2, 345011),\n",
       " (3, 106957),\n",
       " (4, 34459),\n",
       " (5, 10561),\n",
       " (6, 3638),\n",
       " (7, 1261),\n",
       " (8, 494),\n",
       " (9, 238),\n",
       " (10, 106),\n",
       " (11, 50),\n",
       " (13, 34),\n",
       " (12, 26),\n",
       " (14, 23),\n",
       " (16, 16),\n",
       " (18, 14),\n",
       " (27, 10),\n",
       " (20, 9),\n",
       " (32, 9),\n",
       " (19, 9),\n",
       " (34, 8),\n",
       " (15, 8),\n",
       " (40, 7),\n",
       " (26, 7),\n",
       " (24, 7),\n",
       " (25, 7),\n",
       " (17, 7),\n",
       " (46, 6),\n",
       " (28, 6),\n",
       " (37, 6),\n",
       " (21, 6),\n",
       " (22, 5),\n",
       " (23, 5),\n",
       " (29, 5),\n",
       " (44, 4),\n",
       " (30, 4),\n",
       " (31, 4),\n",
       " (39, 4),\n",
       " (33, 4),\n",
       " (41, 4),\n",
       " (55, 4),\n",
       " (56, 3),\n",
       " (48, 3),\n",
       " (50, 3),\n",
       " (36, 3),\n",
       " (63, 3),\n",
       " (49, 3),\n",
       " (57, 3),\n",
       " (51, 3),\n",
       " (71, 3),\n",
       " (45, 3),\n",
       " (43, 3),\n",
       " (61, 2),\n",
       " (47, 2),\n",
       " (35, 2),\n",
       " (65, 2),\n",
       " (88, 1),\n",
       " (66, 1),\n",
       " (58, 1),\n",
       " (64, 1),\n",
       " (42, 1),\n",
       " (82, 1),\n",
       " (104, 1),\n",
       " (38, 1),\n",
       " (80, 1),\n",
       " (140, 1),\n",
       " (92, 1),\n",
       " (54, 1),\n",
       " (53, 1),\n",
       " (91, 1),\n",
       " (131, 1),\n",
       " (75, 1),\n",
       " (127, 1),\n",
       " (135, 1),\n",
       " (113, 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_counts = text.filter(lambda line: len(line) > 0) \\\n",
    "    .filter(lambda line: re.findall('^(?!URL).*', line)) \\\n",
    "    .flatMap(tokenize_chunk_parse) \\\n",
    "    .map(lambda phrase: (len(phrase), 1)) \\\n",
    "    .reduceByKey(lambda x, y: x+y) \\\n",
    "    .sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "np_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "<div style=\"text-align: right\"><sub>Exercise adapted and modified from UMSI homework assignment for SIADS 516.</sub></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
