{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRJob Exercise\n",
    "MRJob exercise to show and explore the concept of MapReduce using large text corpora from Project Gutenberg and basic MR word parsing applications.\n",
    "\n",
    "`%%time` functions included, but results may vary by operating system configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell uses the magic command `%%file`, which will allow us to write the contents of the cell out to a file. This is required in order to use MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_count.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "  ### input: self, in_key, in_value\n",
    "  def mapper(self, _, line):\n",
    "    yield \"chars\", len(line)           # how many characters in a line, regardless of alphanumeric\n",
    "    yield \"words\", len(line.split())   # split line into words based on spaces, to be mapped, return length of list\n",
    "    yield \"lines\", 1                   # returns number of lines analyzed on pass, each line one pass (value of 1)\n",
    "\n",
    "  ### input: self, in_key from mapper, in_value from mapper\n",
    "  def reducer(self, key, values):\n",
    "    yield key, sum(values)             # for each key (\"chars\", \"words\", \"lines\") return sum of values for each key\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `word_count.py` on short text file as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033835.919762\n",
      "Running step 1 of 1...\n",
      "job output is in /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033835.919762/output\n",
      "Streaming final output from /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033835.919762/output...\n",
      "\"words\"\t1822\n",
      "\"lines\"\t200\n",
      "\"chars\"\t10653\n",
      "Removing temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033835.919762...\n",
      "CPU times: user 4.79 ms, sys: 5.63 ms, total: 10.4 ms\n",
      "Wall time: 349 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python word_count.py ../assets/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `word_count.py` on extremely long text file containing the works of William Shakespeare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033836.265327\n",
      "Running step 1 of 1...\n",
      "job output is in /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033836.265327/output\n",
      "Streaming final output from /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033836.265327/output...\n",
      "\"words\"\t901325\n",
      "\"lines\"\t124456\n",
      "\"chars\"\t5333743\n",
      "Removing temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033836.265327...\n",
      "CPU times: user 20.4 ms, sys: 10.4 ms, total: 30.8 ms\n",
      "Wall time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python word_count.py ../assets/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configs mean that we did not make any changes. It creats a temporary directory to do all the work and calculations, then outputs <b> 5,333,743 characters, 901,325 words, and 124,456 lines</b>, then deletes the temp directory and filename.\n",
    "__________________________\n",
    "Now, using a slightly more complicated example to extract the most used words, and integrate the concept of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_used_word.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_used_word.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   #combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                yield (word.lower(), 1)\n",
    "\n",
    "# INCLUSION/EXCLUSION OF COMBINER CODE DISCUSSED BELOW\n",
    "#     def combiner_count_words(self, word, counts):\n",
    "#         # optimization: sum the words we've seen so far\n",
    "#         yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostUsedWord.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yield statement in the `mapper_get_words()` function yields the word and a count of 1, provided the word is not in the list \"STOPWORDS\".\n",
    "\n",
    "The yield statement in the `combiner_count_words()` function yields the sum of all of the counts of each word (from above, where each pass of a word returns 1, thus a summation creates a total count) up to that point.\n",
    "\n",
    "The yield statement in the `reducer_count_words()` function yields a more readable format of the above for the reducer, essentially (a number_of_occurrences, word) pair in a format that can be sorted.\n",
    "\n",
    "The yield statement in the `reducer_find_max_word()` function yields the word_count_pair with the max number of occurences. This happens after all words have been collected and counted through the previous three steps and operates outside of that, pulling the top result <i>after</i> all counts are final.\n",
    "\n",
    "\n",
    "Now we run the file against the short .txt file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/most_used_word.Allie.20220101.033838.386294\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/most_used_word.Allie.20220101.033838.386294/output\n",
      "Streaming final output from /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/most_used_word.Allie.20220101.033838.386294/output...\n",
      "11\t\"day\"\n",
      "Removing temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/most_used_word.Allie.20220101.033838.386294...\n",
      "0.1792588233947754\n",
      "CPU times: user 5.83 ms, sys: 5.45 ms, total: 11.3 ms\n",
      "Wall time: 410 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python most_used_word.py ../assets/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"day\" is the most used word, with a count of 11.\n",
    "\n",
    "Now let's run it on the Shakespeare file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/most_used_word.Allie.20220101.033838.801353\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/most_used_word.Allie.20220101.033838.801353/output\n",
      "Streaming final output from /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/most_used_word.Allie.20220101.033838.801353/output...\n",
      "5479\t\"thou\"\n",
      "Removing temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/most_used_word.Allie.20220101.033838.801353...\n",
      "3.152860164642334\n",
      "CPU times: user 27 ms, sys: 13.1 ms, total: 40.1 ms\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python most_used_word.py ../assets/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common word is \"thou\" with 5479 instances.\n",
    "\n",
    "The end time for the script <i>with</i> the combiner almost a second and a half more than without it. Both scripts produced the same result. This suggests that the extra step of combining is actually slowing down the efficiency of the script. Instead of combining them at an intermediate point, it may be faster to actually combine the final products in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a script that allows us to find the 10 words with the most syllables from the `t5.churchill.txt` file.\n",
    "\n",
    "This output will be messy as there may be a few that have the same number of syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install syllapy\n",
    "# !pip install MRStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_syllables.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_syllables.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import syllapy\n",
    "import re\n",
    "# WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "WORDS_ONLY_RE = re.compile(r\"[a-zA-Z]+\") # any whitespace or apostrophe, used to split lines below\n",
    "class MRWordSyllables(MRJob):\n",
    "    \n",
    "    STOPWORDS = {'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and',\n",
    "                 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',\n",
    "                 'between', 'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'don',\n",
    "                 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have',\n",
    "                 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his',\n",
    "                 'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'me',\n",
    "                 'more','most', 'my', 'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on',\n",
    "                 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "                 's', 'same', 'she', 'should', 'so', 'some', 'such', 't', 'th', 'than', 'that', 'the',\n",
    "                 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they',\n",
    "                 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was',\n",
    "                 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why',\n",
    "                 'will', 'with', 'you', 'your', 'yours', 'yourself', 'yourselves'}\n",
    "    \n",
    "    word_seen = set()\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        return [MRStep(mapper = self.mapper_get_words,\n",
    "                       combiner = self.combiner_swap_tuple,\n",
    "                       reducer = self.reducer_order_set)]\n",
    "    \n",
    "    def mapper_get_words(self, _, line):\n",
    "        # lower and extract words from each line of data\n",
    "        for word in WORDS_ONLY_RE.findall(line.lower()):\n",
    "            # if the word is not a stopword and has not already been processed\n",
    "            # update word_seen and yield word and syllable count\n",
    "            if word not in self.STOPWORDS and word not in self.word_seen:\n",
    "                self.word_seen.add(word)\n",
    "                yield (word, syllapy.count(word))\n",
    "                \n",
    "    def combiner_swap_tuple(self, word, syllable_count):\n",
    "        # effectively zip syllable_count and word into a single 2-tuple\n",
    "        yield (None, (word, syllable_count))\n",
    "                \n",
    "    def reducer_order_set(self, _, syllable_count_pairs):\n",
    "        # sort the tuples by syllable count and word, storing the top 10\n",
    "        sorted_list = sorted(syllable_count_pairs,\n",
    "                             key=lambda pairs: (pairs[1], pairs[0]))[-1:-11:-1]\n",
    "        \n",
    "        # iterate the sorted list extracting each list element\n",
    "        # yielding each item from the element\n",
    "        for (word, syllables) in sorted_list:\n",
    "            yield (word, syllables[0])\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRWordSyllables.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give some reference, we will look at how large the `t5.churchill.txt` file is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033842.197317\n",
      "Running step 1 of 1...\n",
      "job output is in /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033842.197317/output\n",
      "Streaming final output from /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033842.197317/output...\n",
      "\"words\"\t1671473\n",
      "\"lines\"\t189685\n",
      "\"chars\"\t9160853\n",
      "Removing temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_count.Allie.20220101.033842.197317...\n",
      "CPU times: user 27.3 ms, sys: 13.2 ms, total: 40.4 ms\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python word_count.py ../assets/gutenberg/t5.churchill.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the Churchill output with the Shakespeare output:\n",
    "\n",
    "||Shakespeare|Churchill|\n",
    "|----|:---:|:---:|\n",
    "|characters|5,333,743|9,160,853|\n",
    "|words|901,325|1,671,473|\n",
    "|lines|124,456|189,685|\n",
    "\n",
    "We can see that the Churchill text is materially longer than the Shakespeare text, and may take longer to process. (Noting, of course, that these are not necessarily \"long\" texts in relation to some corpora that one may need to work with.)\n",
    "\n",
    "With that said, let's look at the top 10 syllable words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_syllables.Allie.20220101.033845.460481\n",
      "Running step 1 of 1...\n",
      "job output is in /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_syllables.Allie.20220101.033845.460481/output\n",
      "Streaming final output from /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_syllables.Allie.20220101.033845.460481/output...\n",
      "\"overcapitalization\"\t8\n",
      "\"incommunicability\"\t8\n",
      "\"unenforceability\"\t7\n",
      "\"overcapitalized\"\t7\n",
      "\"materialistically\"\t7\n",
      "\"invulnerability\"\t7\n",
      "\"interrogatively\"\t7\n",
      "\"infinitesimally\"\t7\n",
      "\"indissolubility\"\t7\n",
      "\"indispensability\"\t7\n",
      "Removing temp directory /var/folders/7_/nkpwttfs0l398qx_653rtr2w0000gn/T/word_syllables.Allie.20220101.033845.460481...\n",
      "1.5923857688903809\n",
      "CPU times: user 19.7 ms, sys: 10.2 ms, total: 30 ms\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python word_syllables.py ../assets/gutenberg/t5.churchill.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "<div style=\"text-align: right\"><sub>Exercise adapted and modified from UMSI homework assignment for SIADS 516.</sub></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
