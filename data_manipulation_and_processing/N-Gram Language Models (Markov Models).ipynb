{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram language models + Markov Models\n",
    "\n",
    "Exercises exploring the use of the `nltk` library to generate different n-gram utilizations, and exploring applications of Markov Models.\n",
    "_________________________\n",
    "\n",
    "#### First, load the dataset:\n",
    "\n",
    "As the first step towards imitating Shakespeare's writing, you will create a function called `load_data` that loads his original *Sonnets* from `assets/gutenberg/THE_SONNETS.txt`. This function should accomplish the following: \n",
    "\n",
    "* **Extract sentences from the data file.** Of course, depending on the nature of the task at hand, what constitutes a *sentence* can vary. In the context of this assignment, we will define a sentence as just a line of a sonnet, regardless of the punctuation at end. In addition, we will ignore the boundaries of the sonnets --- that is, we are not dealing with $154$ individual *sonnets* but rather $154 \\times 14 = 2156$ *sentences* (actually only $2155$ sentences, as *Sonnet 99* has [15 lines](https://www.google.com/search?ei=po4hX9jzN4rKtQaL7K-wDg&q=why+is+sonnet+99+15+lines&oq=why+is+sonnet+99+15+lines&gs_lcp=CgZwc3ktYWIQAzoECAAQR1ChGlihGmCqHGgAcAJ4AIABXIgBXJIBATGYAQCgAQGqAQdnd3Mtd2l6wAEB&sclient=psy-ab&ved=0ahUKEwjY3snX3PLqAhUKZc0KHQv2C-YQ4dUDCAw&uact=5) but *Sonnet 126* has only [12](https://www.google.com/search?ei=d4whX6_uH9a4tQbUiISoDA&q=why+is+sonnet+126+only+12+lines&oq=o+thou+my+lovely+boy+who+in+thy+power&gs_lcp=CgZwc3ktYWIQARgBMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHUABYAGCWEWgAcAJ4AIABAIgBAJIBAJgBAKoBB2d3cy13aXrAAQE&sclient=psy-ab)). Finally, make sure that the newline character `\\n` at the end of each line is dropped. \n",
    "\n",
    "\n",
    "* **Tokenise each extracted sentence.** While it's ambiguous what a sentence is, what constitutes a \"*word*\" is even more task-dependent. Do punctuations count as \"words\"? Are two \"words\" with the same spelling but different casing considered identical? Since what a text file contains is nothing more than a squence of characters, there are many possible ways of grouping these characters to form \"words\" that are subsequently taken as input by a program. To distinguish what's actually taken as input by a program from a linguistic word, we call the former a *token*. The process of producing a list of tokens out of a sentence is then called *tokenisation*. At this step, you will first lower-case each sentence extracted from the previous step entirely and then tokenise each lower-cased sentence. You may use any tokeniser of your choice, such as `word_tokenize` from the `nltk` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure nltk\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'fairest', 'creatures', 'we', 'desire', 'increase', ',']\n",
      "['that', 'thereby', 'beauty', '’', 's', 'rose', 'might', 'never', 'die', ',']\n",
      "...\n",
      "['came', 'there', 'for', 'cure', 'and', 'this', 'by', 'that', 'i', 'prove', ',']\n",
      "['love', '’', 's', 'fire', 'heats', 'water', ',', 'water', 'cools', 'not', 'love', '.']\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load text data from a file and produce a list of token lists\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    filename = '../assets/gutenberg/THE_SONNETS.txt'\n",
    "    \n",
    "    f = open(filename,'r')\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        tokens = nltk.tokenize.word_tokenize(line.lower())\n",
    "        if len(tokens) > 1:\n",
    "            sentences.append(tokens)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "#len(load_data()) #2155\n",
    "print(load_data()[0])\n",
    "print(load_data()[1])\n",
    "print('...')\n",
    "print(load_data()[-2])\n",
    "print(load_data()[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary of unique tokens\n",
    "\n",
    "Will use `<s>` and `</s>` in vocabulary because we are using them to pad n-gram language models. Token storage order will not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Take a list of sentences and return a vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    result = {x for l in sentences for x in l}\n",
    "    result = list(result)\n",
    "    result.extend(['<s>', '</s>'])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate all n-grams\n",
    "\n",
    "Two-step process:\n",
    "\n",
    "* **Pad each sentence with `<s>` and `</s>` for $n \\geq 2$**.\n",
    "* **Generate $n$-grams on the padded sentences**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams(n, sentences):\n",
    "    \"\"\"\n",
    "    Take a list of unpadded sentences and create all n-grams as specified by the argument \"n\" for each sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    all_ngrams = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        if len(sent) >= 2:\n",
    "            new_sent = list(nltk.lm.preprocessing.pad_both_ends(sent,n))\n",
    "            all_ngrams.append(list(nltk.ngrams(new_sent,n)))\n",
    "    \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', '<s>', 'from'),\n",
       " ('<s>', '<s>', 'from', 'fairest'),\n",
       " ('<s>', 'from', 'fairest', 'creatures'),\n",
       " ('from', 'fairest', 'creatures', 'we'),\n",
       " ('fairest', 'creatures', 'we', 'desire'),\n",
       " ('creatures', 'we', 'desire', 'increase'),\n",
       " ('we', 'desire', 'increase', ','),\n",
       " ('desire', 'increase', ',', '</s>'),\n",
       " ('increase', ',', '</s>', '</s>'),\n",
       " (',', '</s>', '</s>', '</s>')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_n = 4\n",
    "res_sents = load_data()\n",
    "res_ngrams = build_ngrams(res_n, res_sents)\n",
    "res_ngrams[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "## Token Prediction with Markov Chains\n",
    "Let's train an n-gram language model that will \"imitate\" William Shakespeare's writing.\n",
    "\n",
    "First, let's assume we are working with bi-grams here (first-order Markov)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run functions\n",
    "sentence_data = load_data()\n",
    "vocab = build_vocab(sentence_data)\n",
    "bigrams = build_ngrams(2, sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my first iteration of this. It is more step-by-step\n",
    "\n",
    "def bigram_next_token(start_tokens=(\"<s>\", ) * 3):\n",
    "    \"\"\"\n",
    "    Take some starting tokens and produce the most likely token that follows under a bi-gram model\n",
    "    \"\"\"\n",
    "    \n",
    "    from collections import Counter\n",
    "    \n",
    "    n = 2\n",
    "    sents = load_data()\n",
    "    bigrams = build_ngrams(n, sents)\n",
    "    \n",
    "    prev_token = start_tokens[-1]\n",
    "    \n",
    "    cnt_lst = [x for b in bigrams for x in set(b) if x[0] == prev_token]\n",
    "    c1 = Counter(cnt_lst)\n",
    "    \n",
    "    tot = 0\n",
    "    for k in c1:\n",
    "        tot += c1[k]\n",
    "    \n",
    "    prob = 0\n",
    "    next_token = ''\n",
    "    for ele in c1:\n",
    "        k = ele[-1]\n",
    "        v = c1[ele] / tot\n",
    "        #dct[k] = v\n",
    "        if v > prob:\n",
    "            prob = v\n",
    "            next_token = k\n",
    "    \n",
    "    return next_token, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('and', 0.1122969837587007)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function test for after <s>,<s>,<s>,...\n",
    "bigram_next_token(start_tokens=(\"<s>\", ) * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a later, cleaner iteration\n",
    "\n",
    "def bigram_next_token(start_tokens=(\"<s>\", ) * 3):\n",
    "    \"\"\"\n",
    "    Take some starting tokens and produce the most likely token that follows under a bi-gram model\n",
    "    \"\"\"\n",
    "    global bigrams\n",
    "    \n",
    "    bigram_occurences = [grams[0] for grams in bigrams]\n",
    "    bigram_set = set(bigram_occurences)\n",
    "    start_count = len(bigram_occurences)\n",
    "    bigram_dict = {}\n",
    "    \n",
    "    \n",
    "    for bi in bigram_set:\n",
    "        bigram_dict[bi] = bigram_occurences.count(bi)/start_count\n",
    "\n",
    "    keymax = max(bigram_dict, key=bigram_dict.get)\n",
    "\n",
    "    next_token, prob = keymax[1], bigram_dict[keymax]\n",
    "    \n",
    "    return next_token, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('and', 0.1122969837587007)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should result in the same output\n",
    "bigram_next_token(start_tokens=(\"<s>\", ) * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train an $n$-gram language model\n",
    "\n",
    "Leverage `MLE` class from `nltk.lm`. Will require list of all n-grams for each sentence (`build_ngrams(n, sents)` and a vocabulary (`build_vocab(sents)`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "def train_ngram_lm(n):\n",
    "    \"\"\"\n",
    "    Train a n-gram language model as specified by the argument \"n\"\n",
    "    \"\"\"\n",
    "    \n",
    "    lm = MLE(n)\n",
    "    \n",
    "    sents = load_data()\n",
    "    train = build_ngrams(n, sents)\n",
    "    vocabulary = build_vocab(sents)\n",
    "    \n",
    "    # https://www.kite.com/python/docs/nltk.lm\n",
    "    lm.fit(train, vocabulary)\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set sonnet parameters and write a sonnet using code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which vulgar scandal stamped upon my self almost\n",
      "with means more blessed than my barren rhyme\n",
      "of him i lose thee ,\n",
      "when wasteful war shall statues overturn ,\n",
      "nor taste , nor despised ,\n",
      "to one , hath every one , one\n",
      "if i no more be grieved at that\n",
      "and with old woes new wail my dear\n",
      "at my mistress ’ eyes .\n",
      "lest sorrow lend me words and words express\n",
      "of others ’ works thou dost wake elsewhere\n",
      "and maiden virtue rudely strumpeted ,\n",
      "herein lives wisdom , beauty doth he give\n",
      "or bends with the bett ’ ring things\n"
     ]
    }
   ],
   "source": [
    "# Every time it runs, depending on how drunk it is, a different sonnet is written. \n",
    "\n",
    "# Sonnet criteria:\n",
    "n = 3\n",
    "num_lines = 14\n",
    "num_words_per_line = 8\n",
    "text_seed = [\"<s>\"] * (n - 1)\n",
    "\n",
    "lm = train_ngram_lm(n)\n",
    "\n",
    "# Sonnet model\n",
    "sonnet = []\n",
    "while len(sonnet) < num_lines:\n",
    "    while True:  # keep generating a line until success\n",
    "        try:\n",
    "            line = lm.generate(num_words_per_line, text_seed=text_seed)\n",
    "        except ValueError:  # the generation is not always successful. need to capture exceptions\n",
    "            continue\n",
    "        else:\n",
    "            line = [x for x in line if x not in [\"<s>\", \"</s>\"]]\n",
    "            sonnet.append(\" \".join(line))\n",
    "            break\n",
    "\n",
    "# print sonnet\n",
    "print(\"\\n\".join(sonnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Markov Models for Part-of-Speech (POS) Tagging\n",
    "\n",
    "First, load POS-tagged data for training and testing. The data comes from a [CoNLL-2003 shared task](https://www.clips.uantwerpen.be/conll2003/ner/). The shared task was originally held as a competition for Named Entity Recognition (NER), but the data it provided also includes POS tags that make POS Tagging possible. NER and POS Tagging are very similar tasks and HMMs are capable of handling both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sent_data(data_file, label=True):\n",
    "    \"\"\"\n",
    "    Load tokens (and labels, if allowed) from a data_file\n",
    "    \"\"\"\n",
    "    \n",
    "    tagged_sents = []\n",
    "    \n",
    "    f = open(data_file, 'r')\n",
    "    f = f.read().split('\\n\\n')\n",
    "  \n",
    "    if label == True:\n",
    "        tagged_sents = [[(s.split()[0], s.split()[1]) for s in lst if len(s.split()) > 0] for lst in [sent.split('\\n') for sent in f]]\n",
    "        tagged_sents = [lst for lst in tagged_sents if len(lst) >= 10]\n",
    "        \n",
    "    elif label == False:\n",
    "        tagged_sents = [[s.split()[0] for s in lst if len(s.split()) > 0] for lst in [sent.split('\\n') for sent in f]]\n",
    "        tagged_sents = [lst for lst in tagged_sents if len(lst) >= 10]\n",
    "    \n",
    "    return tagged_sents\n",
    "\n",
    "\n",
    "# load_data('../assets/conll03/eng.train', label=True)\n",
    "# load_data('../assets/conll03/eng.testa', label=True)\n",
    "# load_data('../assets/conll03/eng.testb', label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a training, development and testing sets using load_sent_data above\n",
    "In all the three data files, we only consider \"substantial\" sentences that have at least 10 tokens. Labels <i>not</i> included in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"train\": load_sent_data('../assets/conll03/eng.train', label=True), \n",
    "           \"dev\": load_sent_data('../assets/conll03/eng.testa', label=True), \n",
    "           \"test\": load_sent_data('../assets/conll03/eng.testb', label=False)\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train HMM tagger\n",
    "This cell may take a minute. The `HiddenMarkovModelTagger` function returns the accuracy on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy over 43319 tokens: 89.13\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "hmm_tagger = HiddenMarkovModelTagger.train(labeled_sequence=dataset['train'], \n",
    "                                           test_sequence=dataset['dev']\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is 89.13%. Not bad. Not perfect.\n",
    "\n",
    "Apply it to testing set to see how it does. THe line below tags the first sentence in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SOCCER', 'NN'),\n",
       " ('-', ':'),\n",
       " ('JAPAN', 'NNP'),\n",
       " ('GET', '.'),\n",
       " ('LUCKY', '\"'),\n",
       " ('WIN', 'NNP'),\n",
       " (',', ','),\n",
       " ('CHINA', 'NNP'),\n",
       " ('IN', 'IN'),\n",
       " ('SURPRISE', 'NNP'),\n",
       " ('DEFEAT', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_tagger.tag(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "<div style=\"text-align: right\"><sub>Exercise adapted and modified from UMSI homework assignment for SIADS 632.</sub></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
